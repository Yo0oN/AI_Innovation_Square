{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝 개념\n",
    "### 인공지능 AI의 분류\n",
    "1. 규칙기반 : prolog 사용\n",
    "2. 학습기반 : 머신러닝 ==> 사이킷런 사용\n",
    "              딥러닝 = 머신러닝 + 신경망 ==> 텐서플로우 사용 (시각인지, 언어지능, 음성지능 등등 ...)\n",
    "              많이 알려줄수록 정확도가 올라간다.\n",
    "              \n",
    "[1] 규칙기반 AI : rule-based system , 수동으로 규칙(특징)을 입력하면 기계가 추론, Prolog, LISP\n",
    "\n",
    "[2] 학습기반 AI : 머신 러닝(Machine Learning), 데이터를 입력하면 기계가 스스로 새로운 특징을 학습하고 예측\n",
    "\n",
    "### 머신러닝 분류\n",
    "[1] 지도학습(Supervised Learning) : 답이 주어진 상태에서 학습<br>\n",
    "회귀(Regression)<br>\n",
    "분류(Classification)<br><br>\n",
    "[2] 비지도학습(Unsupervised Learning) : 답을 모르고 학습<br>\n",
    "군집화(Clustering)<br>\n",
    "차원 축소(Dimension Reduction) : PCA(주성분 분석, Pricipal Component Analysis)<br><br>\n",
    "[3] 강화 학습(Reinforcement Learning) : 답을 모르고 있는 상태에서 답을 알아가는 강한 인공지능(자아를 갖음, 인간수준)<br>\n",
    "게임, 알파고(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론과 XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def AND (x1, x2) : \n",
    "    w1, w2, theta = 0.5, 0.5, 0.7\n",
    "    tmp = w1 * x1 + w2 * x2\n",
    "    if tmp <= theta : # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta :\n",
    "        return 1\n",
    "\n",
    "print(AND(0, 0))\n",
    "print(AND(1, 0))\n",
    "print(AND(0, 1))\n",
    "print(AND(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def OR (x1, x2) :\n",
    "    w1, w2, theta = 0.5, 0.5, 0.4\n",
    "    tmp = w1 * x1 + w2 * x2\n",
    "    if tmp <= theta : # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta :\n",
    "        return 1\n",
    "\n",
    "print(OR(0, 0))\n",
    "print(OR(1, 0))\n",
    "print(OR(0, 1))\n",
    "print(OR(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def NAND (x1, x2) : # AND를 반전시킨것\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7\n",
    "    tmp = w1 * x1 + w2 * x2\n",
    "    if tmp <= theta : # 임계값\n",
    "        return 1\n",
    "    elif tmp > theta :\n",
    "        return 0\n",
    "\n",
    "print(NAND(0, 0))\n",
    "print(NAND(1, 0))\n",
    "print(NAND(0, 1))\n",
    "print(NAND(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단층 퍼셉트론의 한계 : XOR의 한계\n",
    "# def XOR (x1, x2) : # AND를 반전시킨것\n",
    "#     w1, w2, theta = 0.5, 0.5, 0.7\n",
    "#     tmp = w1 * x1 + w2 * x2\n",
    "#     if tmp <= theta : # 임계값\n",
    "#         return 1\n",
    "#     elif tmp > theta :\n",
    "#         return 0\n",
    "\n",
    "# print(NAND(0, 0))\n",
    "# print(NAND(1, 0))\n",
    "# print(NAND(0, 1))\n",
    "# print(NAND(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation : 1986년 제프리 힌튼(Geoffrey Hinton)\n",
    "샘플에 대한 신경망의 오차를 다시 출력층에서부터 입력층으로 거꾸로 전파시켜 각 층의 가중치(weight)를 계산하는 방법.<br>이를 통해 weight와 bias를 알맞게 학습할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 다층 퍼셉트론\n",
    "# XOR Problem\n",
    "\n",
    "def XOR (x1, x2) :\n",
    "    s1 = NAND(x1, x2)\n",
    "    s2 = OR(x1, x2)\n",
    "    y = AND(s1, s2)\n",
    "    return y\n",
    "\n",
    "print(XOR(0, 0))\n",
    "print(XOR(0, 1))\n",
    "print(XOR(1, 0))\n",
    "print(XOR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀모델\n",
    "1. 선형회귀 : 1차함수, 직선의 방정식\n",
    "2. 가중치 Weight : 입력 변수가 출력에 영향을 미치는 정도를 설정, 기울기 값, 회귀계수\n",
    "3. 편향 Bias : 기본 출력 값이 활성화 되는 정도를 설정, y절편, 회귀계수\n",
    "4. 비용함수 Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w : -1, cost : 18.666666666666668\n",
      "w : -1, cost : 4.666666666666667\n",
      "w : -1, cost : 0.0\n",
      "w : -1, cost : 4.666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 비용함수의 구현\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cost(x, y, w) :\n",
    "    c = 0\n",
    "    for k in range(len(x)) :\n",
    "        hx = w * x[k] # 예측방정식\n",
    "        loss = (hx - y[k]) ** 2 # (에측값 = 실제값)^2\n",
    "        c += loss\n",
    "    return c / len(x) # 평균 제곱 오차\n",
    "\n",
    "x = [1, 2, 3] # 입력값, 독립변수\n",
    "y = [1, 2, 3] # 답, target, 종속병수\n",
    "\n",
    "print('w : -1, cost :', cost(x, y, -1))\n",
    "print('w : -1, cost :', cost(x, y, 0))\n",
    "print('w : -1, cost :', cost(x, y, 1)) # cost : 0.0\n",
    "print('w : -1, cost :', cost(x, y, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미분 : 순간 변화량, 기울기, x축으로 1만큼 움직였을 때, y축으로 움직인 거리\n",
    "    함수 미분 공식 정리 : f(x) = x^n = n * x^(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression \n",
    "# 알고리즘 구현 : 비용함수와 경사 하강법 알고리즘 함수 구현\n",
    "# (1) 비용함수 구현\n",
    "def cost(x,y,w):\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]          # 예측 방정식\n",
    "        loss = (hx - y[k])**2  # (예측값 - 실제값)^2\n",
    "        c += loss\n",
    "    return c/len(x)           # 평균 제곱 오차 \n",
    "\n",
    "# (2) 경사 하강법 알고리즘 함수 구현\n",
    "def gradient_descent(x,y,w):\n",
    "    c = 0\n",
    "    for i in range(len(x)):\n",
    "        hx = w*x[i]\n",
    "        loss = (hx - y[i])*x[i] # 곱하기 2를 생략한 비용함수의 미분 공식\n",
    "        c += loss\n",
    "        # 비용함수의 미분 : cost(w) = (w*x[i] - y[i])^2 의 미분\n",
    "        # (a + b)^2 = a^2 + 2*a*b + b^2 \n",
    "        # cost(w) = w^2 * x[i]^2 - 2 * w * x[i] * y[i] + y[i]^2\n",
    "        # cost'(w) = 2*w*x[i]^2 - 2*x[i]*y[i] = 2*x[i]*(w*x[i] - y[i]) ==> (hx-y[i])*x[i]\n",
    "\n",
    "    return c/len(x) \n",
    "\n",
    "# (3) 학습(fit) 함수 구현\n",
    "def fit(x,y):\n",
    "    print('--------start training!!')\n",
    "    w,old = 10,100\n",
    "    for i in range(1000):\n",
    "        c = cost(x,y,w)\n",
    "        grad = gradient_descent(x,y,w)\n",
    "        w -= 0.1*grad  # 0.1 :학습률(learning rate), 하이퍼 파라메터, 가중치 의 업데이트 실행\n",
    "        print('[%03d]'%i,'cost:',c,'old:',old,'weight:',w)\n",
    "        if c >= old and abs(c - old)< 1.0e-15: # cost가 1.0e-15 값 보다도 더 줄지 않을 때\n",
    "            break\n",
    "        old = c\n",
    "    print('--------end training!!')\n",
    "    return w\n",
    "\n",
    "# (4) 예측(predict) 함수 구현     \n",
    "def predict(w,x):\n",
    "    hx = w*np.array(x)\n",
    "    return list(hx)\n",
    "\n",
    "# (5) 정확도(평가지표) 측정 함수 구현 : 정확도 검증(validation)\n",
    "# <1> 분류(classification) 일때 : 정확도(%)\n",
    "def get_accuarcy(w,x_test,y_test):\n",
    "    y_predict = predict(w,x_test)\n",
    "    print(y_predict)\n",
    "    correct = 0\n",
    "    for k,_ in enumerate(y_test):\n",
    "        if y_test[k] == y_predict[k]:\n",
    "            correct += 1\n",
    "    accuracy = round(correct/len(y_test),2)\n",
    "    return accuracy\n",
    "\n",
    "# <2> 회귀(Linear Regression) 일때 : RMSE(Root Mean Squared Error,평균 제곱근 오차)\n",
    "def get_rmse(w,x_test,y_test):\n",
    "    y_predict = predict(w,x_test)\n",
    "    print(y_predict)\n",
    "    squared_error = 0\n",
    "    for k,_ in enumerate(y_test):\n",
    "        squared_error += (y_predict[k] - y_test[k])**2\n",
    "    mse = squared_error/len(y_test)    \n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------start training!!\n",
      "[000] cost: 704.0 old: 100 weight: 1.1999999999999993\n",
      "[001] cost: 7.040000000000012 old: 704.0 weight: 2.08\n",
      "[002] cost: 0.07040000000000013 old: 7.040000000000012 weight: 1.992\n",
      "[003] cost: 0.0007039999999999871 old: 0.07040000000000013 weight: 2.0008\n",
      "[004] cost: 7.03999999999845e-06 old: 0.0007039999999999871 weight: 1.99992\n",
      "[005] cost: 7.040000000016923e-08 old: 7.03999999999845e-06 weight: 2.0000080000000002\n",
      "[006] cost: 7.040000000546988e-10 old: 7.040000000016923e-08 weight: 1.9999992\n",
      "[007] cost: 7.040000000689097e-12 old: 7.040000000546988e-10 weight: 2.00000008\n",
      "[008] cost: 7.039999989746739e-14 old: 7.040000000689097e-12 weight: 1.999999992\n",
      "[009] cost: 7.039999978378055e-16 old: 7.039999989746739e-14 weight: 2.0000000008\n",
      "[010] cost: 7.040001164984472e-18 old: 7.039999978378055e-16 weight: 1.99999999992\n",
      "[011] cost: 7.040001164984472e-20 old: 7.040001164984472e-18 weight: 2.000000000008\n",
      "[012] cost: 7.039830636607046e-22 old: 7.040001164984472e-20 weight: 1.9999999999992\n",
      "[013] cost: 7.040612264052197e-24 old: 7.039830636607046e-22 weight: 2.00000000000008\n",
      "[014] cost: 7.028750665519215e-26 old: 7.040612264052197e-24 weight: 1.999999999999992\n",
      "[015] cost: 6.888333424389875e-28 old: 7.028750665519215e-26 weight: 2.000000000000001\n",
      "[016] cost: 7.257520328033308e-30 old: 6.888333424389875e-28 weight: 2.0\n",
      "[017] cost: 0.0 old: 7.257520328033308e-30 weight: 2.0\n",
      "[018] cost: 0.0 old: 0.0 weight: 2.0\n",
      "--------end training!!\n",
      "weight: 2.0\n",
      "y_predict [12.0, 14.0, 16.0, 18.0, 20.0]\n",
      "[2.0, 4.0, 6.0]\n",
      "Accuracy: 0.67\n",
      "[2.0, 4.0, 6.0]\n",
      "RMSE: 0.5773502691896257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 머신러닝 사용자가 구현 할 부분: 모델 구현\n",
    "\n",
    "# (1) fit() 함수 호출하여 학습 수행\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "w = fit(x_train,y_train)\n",
    "print('weight:',w)\n",
    "\n",
    "# (2) predict() 함수 호출하여 예측\n",
    "x_predict = [6,7,8,9,10]   # x값 만 사용\n",
    "y_predict = predict(w,x_predict)\n",
    "print('y_predict',y_predict)  # [12.0, 14.0, 16.0, 18.0, 20.0]\n",
    "\n",
    "# (3) 정확도 측정 함수 호출하여 평가\n",
    "# 분류 모델인 경우\n",
    "x_test = [1,2,3]\n",
    "# y_test = [2,4,6]\n",
    "y_test = [2,4,7]\n",
    "accuracy = get_accuarcy(w,x_test,y_test)\n",
    "print('Accuracy:',accuracy)  # Accuracy: 0.67\n",
    "\n",
    "# 회귀 모델인 경우\n",
    "x_test = [1,2,3]\n",
    "# y_test = [2,4,6]\n",
    "y_test = [2,4,7]\n",
    "rmse = get_rmse(w,x_test,y_test)\n",
    "print('RMSE:',rmse) # RMSE: 0.5773502691896257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
